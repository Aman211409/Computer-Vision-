{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Object Detection using Faster R-CNN**\n\n**Hi everyone!**\n\nIn this notebook, I will show you how we can train and finetune Faster R-CNN and Mask R-CNN models for object detection of fruit image detection dataset. If you want to brush up about what is Faster RCNN, [here](https://whatdhack.medium.com/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd) is an awesome medium article on the same.","metadata":{}},{"cell_type":"markdown","source":"## Install Libraries\n\nSince a lot of code for object detection is same and has to be rewritten by everyone, torchvision contributers have provided us with helper codes for training, evaluation and transformations.\n\nLet's clone the repo and copy the libraries into working directory\n\n","metadata":{}},{"cell_type":"code","source":"!pip install pycocotools --quiet\n!git clone https://github.com/pytorch/vision.git\n!git checkout v0.3.0\n\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:06:06.11057Z","iopub.execute_input":"2021-07-09T13:06:06.110996Z","iopub.status.idle":"2021-07-09T13:06:30.152081Z","shell.execute_reply.started":"2021-07-09T13:06:06.110908Z","shell.execute_reply":"2021-07-09T13:06:30.150958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches \nimport cv2\nimport torch\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom torchvision.models.detection import *\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n# For image augmentations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# As the data directory contains .xml files\nfrom xml.etree import ElementTree as et\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:07:44.132426Z","iopub.execute_input":"2021-07-09T13:07:44.132783Z","iopub.status.idle":"2021-07-09T13:07:44.140491Z","shell.execute_reply.started":"2021-07-09T13:07:44.132748Z","shell.execute_reply":"2021-07-09T13:07:44.139502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining the files directory and testing directory\nfiles_dir = '../input/fruit-images-for-object-detection/train_zip/train'\ntest_dir = '../input/fruit-images-for-object-detection/test_zip/test'","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:06:33.255585Z","iopub.execute_input":"2021-07-09T13:06:33.255894Z","iopub.status.idle":"2021-07-09T13:06:33.263387Z","shell.execute_reply.started":"2021-07-09T13:06:33.255867Z","shell.execute_reply":"2021-07-09T13:06:33.262448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Faster R-CNN","metadata":{}},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"class FruitImageDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, files_dir, width, height, transforms=None):\n        self.files_dir = files_dir\n        self.width = width\n        self.height = height\n        self.transforms = transforms  # If transformation is required, when transforms is not None\n        \n        self.classes_ = [_, 'apple', 'orange', 'banana']  # Defining classes, a blank class is given for the background\n        \n        self.images = [img for img in sorted(os.listdir(files_dir)) if img[-4:]=='.jpg']\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.files_dir, img_name)\n        \n        # Reading the image\n        img = cv2.imread(img_path)\n        \n        # Defining width and height\n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        # Converting image to RGB channel and normalizing the image\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img = cv2.resize(img, (self.width, self.height), cv2.INTER_AREA)\n        img /= 255.0\n        \n        annot_name = img_name[:-4] + '.xml'\n        annot_path = os.path.join(self.files_dir, annot_name)\n        \n        # Boxes to store the coordinate points of the bboxes\n        boxes, labels = [], []\n        \n        tree = et.parse(annot_path)\n        root = tree.getroot()\n        \n        # Box coordinates are extracted from the XML files for the given image size\n        for member in root.findall('object'):\n            labels.append(self.classes_.index(member.find('name').text))\n            \n            xmin = float(member.find('bndbox').find('xmin').text)\n            xmax = float(member.find('bndbox').find('xmax').text)\n            ymin = float(member.find('bndbox').find('ymin').text)\n            ymax = float(member.find('bndbox').find('ymax').text)\n            \n            x_min = (xmin/wt)*self.width\n            x_max = (xmax/wt)*self.width\n            y_min = (ymin/ht)*self.height\n            y_max = (ymax/ht)*self.height\n            \n            boxes.append([x_min, y_min, x_max, y_max])\n            \n        # Conversion to Tensors   \n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])  # Calculating area of the boxes\n        \n        iscrowd = torch.zeros((boxes.shape[0], ), dtype=torch.int64)\n        \n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        \n        image_id = torch.tensor([idx])\n        \n        target = {'boxes': boxes, 'area': area, 'labels': labels, \n                'iscrowd': iscrowd, 'image_id':image_id}\n        \n        if self.transforms:\n            sample = self.transforms(image = img,\n                                    bboxes = target['boxes'],\n                                    labels = labels)\n            \n            img = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n        return img, target","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-09T13:11:04.28878Z","iopub.execute_input":"2021-07-09T13:11:04.289162Z","iopub.status.idle":"2021-07-09T13:11:04.3089Z","shell.execute_reply.started":"2021-07-09T13:11:04.289127Z","shell.execute_reply":"2021-07-09T13:11:04.307788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Observing the dataset without any transformation\ndataset = FruitImageDataset(files_dir, 224, 224)\nprint('length of dataset = ', len(dataset), '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:06:33.285265Z","iopub.execute_input":"2021-07-09T13:06:33.285667Z","iopub.status.idle":"2021-07-09T13:06:33.472877Z","shell.execute_reply.started":"2021-07-09T13:06:33.285628Z","shell.execute_reply":"2021-07-09T13:06:33.471948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observing and Visualizing the bounding boxes","metadata":{}},{"cell_type":"code","source":"img, target = dataset[78]\nprint(img.shape, '\\n',target)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:06:33.475618Z","iopub.execute_input":"2021-07-09T13:06:33.475879Z","iopub.status.idle":"2021-07-09T13:06:33.616955Z","shell.execute_reply.started":"2021-07-09T13:06:33.475851Z","shell.execute_reply":"2021-07-09T13:06:33.616055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we can see the image at the 78th index comprises of 4 boxes","metadata":{}},{"cell_type":"code","source":"def plot_img_bbox(img, target):\n    \n    # plot the image and bboxes\n    # Bounding boxes are defined as follows: x-min y-min width height\n    fig, a = plt.subplots(1,1)\n    fig.set_size_inches(5,5)\n    a.imshow(img)\n    \n    for box in (target['boxes']):\n        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = patches.Rectangle((x, y),\n                                 width, height,\n                                 linewidth = 2,\n                                 edgecolor = 'r',\n                                 facecolor = 'none')\n\n        # Draw the bounding box on top of the image\n        a.add_patch(rect)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:06:33.618385Z","iopub.execute_input":"2021-07-09T13:06:33.61874Z","iopub.status.idle":"2021-07-09T13:06:33.625643Z","shell.execute_reply.started":"2021-07-09T13:06:33.618702Z","shell.execute_reply":"2021-07-09T13:06:33.624454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, target = dataset[25]\nplot_img_bbox(img, target)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:06:33.628592Z","iopub.execute_input":"2021-07-09T13:06:33.62929Z","iopub.status.idle":"2021-07-09T13:06:33.863581Z","shell.execute_reply.started":"2021-07-09T13:06:33.629249Z","shell.execute_reply":"2021-07-09T13:06:33.862685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, target = dataset[78]\nplot_img_bbox(img, target)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:06:33.864977Z","iopub.execute_input":"2021-07-09T13:06:33.865315Z","iopub.status.idle":"2021-07-09T13:06:34.04401Z","shell.execute_reply.started":"2021-07-09T13:06:33.865279Z","shell.execute_reply":"2021-07-09T13:06:34.042977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, target = dataset[120]\nplot_img_bbox(img, target)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:06:34.045569Z","iopub.execute_input":"2021-07-09T13:06:34.045912Z","iopub.status.idle":"2021-07-09T13:06:34.231344Z","shell.execute_reply.started":"2021-07-09T13:06:34.045877Z","shell.execute_reply":"2021-07-09T13:06:34.230288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development","metadata":{}},{"cell_type":"markdown","source":"### Initializing the model\n\nWe may use both the models that is **Faster R-CNN** or **Mask R-CNN** and compare which model gives us a better **Average Precision (AP)**. If possible, we can also try out making an ensemble of the 2 models. ","metadata":{}},{"cell_type":"code","source":"def get_model(num_classes, modelName):\n    \n    # Loading the pre-trained model\n    if modelName == 'fastcnn':\n        model = fasterrcnn_resnet50_fpn(pretrained=True)\n        in_features = model.roi_heads.box_predictor.cls_score.in_features\n        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n        return model\n    \n    elif modelName == 'maskcnn':\n        model = maskrcnn_resnet50_fpn(pretrained=True)\n        in_features = model.roi_heads.box_predictor.cls_score.in_features\n        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n        return model","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:06:34.233159Z","iopub.execute_input":"2021-07-09T13:06:34.233754Z","iopub.status.idle":"2021-07-09T13:06:34.243663Z","shell.execute_reply.started":"2021-07-09T13:06:34.233683Z","shell.execute_reply":"2021-07-09T13:06:34.242667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentaion\n\nThe process for augmenting the data for object detection is different from object classification as we need to ensure that the final bounding box still aligns with the object after transforming. ","metadata":{}},{"cell_type":"code","source":"def get_transform(train=True):\n    if train:\n        return A.Compose([\n            A.HorizontalFlip(0.5),\n            ToTensorV2(p=0.1),     # ToTensorV2 converts image to PyTorch tensor without dividing by 255\n        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n    else:\n        return A.Compose([\n            ToTensorV2(p=0.1),     # ToTensorV2 converts image to PyTorch tensor without dividing by 255\n        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:06:34.245691Z","iopub.execute_input":"2021-07-09T13:06:34.248066Z","iopub.status.idle":"2021-07-09T13:06:34.263232Z","shell.execute_reply.started":"2021-07-09T13:06:34.24802Z","shell.execute_reply":"2021-07-09T13:06:34.262231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Preparation","metadata":{}},{"cell_type":"code","source":"test_split = 0.2\n\n# Loading the training and the testing data with all tghe transformations\ndataset_train = FruitImageDataset(files_dir, 480, 480, transforms=get_transform(train=True))\ndataset_test = FruitImageDataset(files_dir, 480, 480, transforms=get_transform(train=False))\n\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\n\n# Train test split\ntsize = int(len(dataset) * test_split) # Getting the splitting index\ndataset_train = torch.utils.data.Subset(dataset_train, indices[:-tsize])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n\n# Defining dataloaders\ndataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True,\n                                              num_workers=4, collate_fn=utils.collate_fn)  # Imported form helper library\ndataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=8, shuffle=True,\n                                              num_workers=4, collate_fn=utils.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:11:14.782344Z","iopub.execute_input":"2021-07-09T13:11:14.782692Z","iopub.status.idle":"2021-07-09T13:11:14.796666Z","shell.execute_reply.started":"2021-07-09T13:11:14.782659Z","shell.execute_reply":"2021-07-09T13:11:14.795761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training\n### Model Configuration","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nnum_classes = 4 # Can try by changing to 3 as 1 class is devoted for background\nnum_epochs = 9\n\ndef start_training(modelName, num_epochs, num_classes):\n    model = get_model(num_classes, modelName)\n    model.to(device)\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.005)\n    \n    # Learning rate decreases by 10 every 5 epochs\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n    \n    for epoch in range(num_epochs):\n        train_one_epoch(model, optimizer, dataloader_train, device, epoch, print_freq=5)\n        lr_scheduler.step()\n        evaluate(model, dataloader_test, device=device)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:19:43.355289Z","iopub.execute_input":"2021-07-09T13:19:43.355652Z","iopub.status.idle":"2021-07-09T13:19:43.364109Z","shell.execute_reply.started":"2021-07-09T13:19:43.35562Z","shell.execute_reply":"2021-07-09T13:19:43.363306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fast_rcnn = start_training('fastcnn', num_epochs, num_classes)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:27:14.298077Z","iopub.execute_input":"2021-07-09T13:27:14.298436Z","iopub.status.idle":"2021-07-09T13:32:57.505039Z","shell.execute_reply.started":"2021-07-09T13:27:14.298403Z","shell.execute_reply":"2021-07-09T13:32:57.504131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have achieved an **Average Precision (AP)** of **0.940**.","metadata":{}},{"cell_type":"markdown","source":"## Decoding Predictions\n\nIf and when a model predicts many bounding boxes for a single object, we apply **Non Max Supression** to take out the overlapping boxes. <br>\nWe will be using torchvision's 'nms' method to do so.","metadata":{}},{"cell_type":"code","source":"def apply_nms(prediction, threshold):\n    # torchvision returns the indices of the boxes to keep\n    keep = torchvision.ops.nms(prediction['boxes'], prediction['scores'], threshold)\n    \n    final_prediction = prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\n# Function to convert a torch tensor to a PIL Image\ndef tensorToPIL(img):\n    return transforms.ToPILImage()(img).convert('RGB')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:41:36.517115Z","iopub.execute_input":"2021-07-09T13:41:36.517472Z","iopub.status.idle":"2021-07-09T13:41:36.525434Z","shell.execute_reply.started":"2021-07-09T13:41:36.517435Z","shell.execute_reply":"2021-07-09T13:41:36.524586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fast R-CNN model testing","metadata":{}},{"cell_type":"code","source":"# pick one image from the test set\nimg, target = dataset_test[5]\n\n# put the model in evaluation mode\nfast_rcnn.eval()\nwith torch.no_grad():\n    prediction = fast_rcnn([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['labels']))\nprint('real #boxes: ', len(target['labels']))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:42:51.558308Z","iopub.execute_input":"2021-07-09T13:42:51.558659Z","iopub.status.idle":"2021-07-09T13:42:51.652309Z","shell.execute_reply.started":"2021-07-09T13:42:51.55863Z","shell.execute_reply":"2021-07-09T13:42:51.651512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We received an output that our model  returns 5 boxes extra in the test image at index 5 ","metadata":{}},{"cell_type":"code","source":"print('EXPECTED OUTPUT')\nplot_img_bbox(tensorToPIL(img), target)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:44:16.65582Z","iopub.execute_input":"2021-07-09T13:44:16.656138Z","iopub.status.idle":"2021-07-09T13:44:16.812232Z","shell.execute_reply.started":"2021-07-09T13:44:16.656108Z","shell.execute_reply":"2021-07-09T13:44:16.811195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('MODEL OUTPUT')\nplot_img_bbox(tensorToPIL(img), prediction)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:44:30.167725Z","iopub.execute_input":"2021-07-09T13:44:30.168041Z","iopub.status.idle":"2021-07-09T13:44:30.348564Z","shell.execute_reply.started":"2021-07-09T13:44:30.16801Z","shell.execute_reply":"2021-07-09T13:44:30.347596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that our model did return 6 boxes instead of 1.\n\n#### Let's apply NMS","metadata":{}},{"cell_type":"code","source":"nms_preds = apply_nms(prediction, threshold=0.2)\nprint('NMS APPLIED MODEL OUTPUT')\nplot_img_bbox(tensorToPIL(img), nms_preds)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:46:05.036973Z","iopub.execute_input":"2021-07-09T13:46:05.037293Z","iopub.status.idle":"2021-07-09T13:46:05.193829Z","shell.execute_reply.started":"2021-07-09T13:46:05.037261Z","shell.execute_reply":"2021-07-09T13:46:05.192853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that after applying Non Max Supression, out model returns precisely 2 boxes, whereas in real XML, it was 1. Our model predicts 2 boxes as we can see in the image that there are 2 pieces of the same fruit.","metadata":{}},{"cell_type":"code","source":"test_dataset = FruitImageDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n# pick one image from the test set\nimg, target = test_dataset[10]\n# put the model in evaluation mode\nfast_rcnn.eval()\nwith torch.no_grad():\n    prediction = fast_rcnn([img.to(device)])[0]\n    \nprint('EXPECTED OUTPUT\\n')\nplot_img_bbox(tensorToPIL(img), target)\nprint('MODEL OUTPUT\\n')\nnms_prediction = apply_nms(prediction, threshold=0.01)\n\nplot_img_bbox(tensorToPIL(img), nms_prediction)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:48:46.111452Z","iopub.execute_input":"2021-07-09T13:48:46.111773Z","iopub.status.idle":"2021-07-09T13:48:46.503049Z","shell.execute_reply.started":"2021-07-09T13:48:46.111744Z","shell.execute_reply":"2021-07-09T13:48:46.50205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"fast_rcnn","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:49:53.539768Z","iopub.execute_input":"2021-07-09T13:49:53.540081Z","iopub.status.idle":"2021-07-09T13:49:53.548415Z","shell.execute_reply.started":"2021-07-09T13:49:53.540053Z","shell.execute_reply":"2021-07-09T13:49:53.547549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our model is predicting slices as well, therefore, there is room for improvement. <br>\n1. Extensive use of Albumentations.\n2. Modifying the Optimizer\n3. Modufying the Learning Rate scheduler.\n4. Changing the size of the images.\n5. Changing the backbone of the model.\n6. Using AutoAlbumentations","metadata":{}},{"cell_type":"markdown","source":"# Thank You!\n## The END!","metadata":{}},{"cell_type":"markdown","source":"### Do Upvote this notebook","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}